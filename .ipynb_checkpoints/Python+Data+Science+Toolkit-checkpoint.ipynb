{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Python Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>Importing Functions and Libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we'll need to do in order to work with our data is to import\n",
    "the operating systems and additional libraries that we'll need to use\n",
    "if our code is like a recipe, these imports are like the cooking tools and equipment\n",
    "we'll use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter #if we're importing a file from Excel\n",
    "from pandas import ExcelFile #if we're importing a file from Excel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when we start working with plots/graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>Importing Datasets</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to import our data as a dataframe.\n",
    "This will probably be in an Excel or CSV (comma separated values) format, which we can import from an Excel file with \n",
    "\n",
    "df = pd.read_excel(<font color = 'red'>'file path name'</font>, sheetname = <font color = 'red'>'Sheet, if you want to grab info from any sheet other than the first one (default)'</font>). \n",
    "\n",
    "Or, in a CSV file with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\melanie.shimano\\\\Documents\\\\ECB_Citations.csv\")\n",
    "#download the ECB Citation dataset and copy that file's path name in order to follow along with examples in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you copy the file path name, you'll need to either add an extra '\\' whenever one appears in the file path name, replace the '\\'s with '/'s, or add an r before the quote in your file name pathway. This is because a backwards slash in python has another function.\n",
    "\n",
    "You can get the file path name by holding shift while right-clicking the document, then chosing \"Copy Path\" and pasting that information into your code in ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>Indexing Datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll want to index our dataframe so that we can label our rows and use this as a row identifier.\n",
    "You can set your index to a column name to help identify what you're looking for with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index('ViolationDate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to access this data later, you'll be able to reset the index by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index('CitationNo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>Previewing Datasets</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check that we imported the correct dataset, then we can preview our dataframe by looking at the top 5 rows with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the bottom 5 rows with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a specific number of rows at the top or bottom by putting that number in the parentheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if it looks like most of our data is self-explanatory, we might want to check the kind of data stored in case it's all in strings, etc.\n",
    "We can do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a quick overview of our data by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all numerical value columns in our dataset, or we can look at a particular column's statistics by typing the column's name in quotes in the parentheses. If we look back at our df.info() results, we see that only CitationNo and CouncilDistrict columns are numerical values (floats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are null/NaN (not a number) values in our dataframe, we might want to delete them and their corresponding rows in order to do specific calculations. We can do this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our data isn't in the format that we want (e.g. numbers are stored as a string instead of an integer, etc.), then we can change an entire column's format by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.col_name = df.col_name.astype(np.int64) #changing to integer\n",
    "df.col_name = df.col_name.astype(np.float) #changing to float\n",
    "df.col_name = df.col_name.astype(np.str) #changing to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>Column Operations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same operations that we use with single ints and floats with entire columns. When we do this, we'll probably want to create a new column in our dataframe to hold the values from our new calculations. We do this by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cit_balance']= #column operation that will fill values in new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform column operations, we'll use the dataframe name and column name somewhat like a variable in the format: df['existing_column'].  For example, if we want to subtract the TotalPaid column from the FineAmount column and set the values in a new cit_balance in dataframe df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.FineAmount = df.FineAmount.str.replace('$', '').replace(',', '').astype(np.float64) \n",
    "df.TotalPaid = df.TotalPaid.str.replace('$', '').astype(np.float64)\n",
    "#in order to perform calculations with these values, we need to convert the string objects (see df.info()) to floats (they \n",
    "#have decimals). We define the column in the dataframe to change, remove the $, and change the data type to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at our dataframe's statistics with .describe(), we will see information for all of the columns that we changed to numerical values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values('FineAmount', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to implement the code above before we can perform a new column operation (try to run the code below fist, and you'll get an error message). The above code does the following: \n",
    "1. The first df.FineAmount = redefines the column and dataframe that we will edit\n",
    "2. The second df.FineAmount states which column in the dataframe we are editing\n",
    "3. .str.replace says that in the current string, we are replacing all of the $ with nothing (' ' are empty quotes)\n",
    "4. .astype(npfloat64) says that we are redefining the string as a float\n",
    "\n",
    "We need to remove the $ from our numbers before we redefine the string because characters don't translate to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cit_balance']= df['FineAmount'] - df['TotalPaid']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can do column calculations like this with any operator (e.g. +, -, *, /, etc.), and we can also combine column calculations with regular variable or integer calculations.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['new_column']= 2*df['FineAmount']\n",
    "df['new_column_2'] = (df['cit_balance']/df['FineAmount'])*100 #if we want to know the percentage of fees still owed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate information on columns as a whole. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['col'].sum() #will give us the sum of col\n",
    "df['col'].mean() #will give us the mean of col\n",
    "df['col'].count() #will give us the number of items in col\n",
    "df['col'].std() #will give us the standard deviation of the col values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For example, if we want to know the average citation price\n",
    "df['FineAmount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For example, if we want to set the count of citations as a value in a new column: \n",
    "df.loc['Total']= df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a row at the bottom of our dataframe with the total sums of the columns then we can do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc['Total']=df.sum() #NOTE: this will only give the sum for numerical values\n",
    "#you probably also don't want to do this if you are going to continue to do calculations on the column as a whole\n",
    "#unless you explicitly don't include the last value in the calculatoins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>Split-Apply-Combine</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're dealing with a particularly large or complicated database (most of them), we'll want to use the Split-Apply-Combine strategy for analyzing our data. This means that we'll break up a huge dataset into smaller, more manageable pieces, operate on each piece individually, and then put it all back together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split__: We want to split up our data into meaningful groups. Usually, our dataset will have some sort of features that we'll want to sort by already.  \n",
    "\n",
    "For example, if we look at the dataset in our Github page for ECB Citations, we'll see that we have a lot of information about the citations listed. It might be helpful to calculate the total fine amount in our dataset, but it'll probably be more helpful to calculate the fine amounts by different categories such as Agency, Description, or Block.\n",
    "\n",
    "When we want to split our dataset, we use the pandas function __groupby__, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_df = df.groupby('Agency') #will create a new dataframe, new_df, that groups the citations by Agency\n",
    "new_df = df.groupby('Block') #will create a new dataframe, new_df, that groups the citations by Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Apply & Combine__: Next, we'll want to apply calculations on these groups, and combine the results in a new column in our dataframe, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we simply write: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('Agency').count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make the Agency name the new dataframe index and fill in the Agency count value for every \"cell\" in our dataframe, which we don't want!\n",
    "\n",
    "If we simply want to look at the counts for each agency, then we can create a __Series__, which looks somewhat like a database, but we can't necessarily do calculations or merge this in its current format with a dataframe. In order to get a series, we can write: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Agency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a new column with the Agency counts, then we can do this a few different ways using the transform function (which \"transforms\" the identified column based on our input, where the output is the same size and shape as the original data): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Agency_Count']= df.groupby('Agency').transform(len) #this will input the \"length\" the list of each 'Agency' group in a new column, 'Agency_Count'\n",
    "#NOTE: every row with the same 'Agency' name will have the same 'Agency_Count' number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Agency_Count']= df.groupby(['Agency'])['Description'].transform('count') \n",
    "#this will count the number of 'Descriptions' listed for each 'Agency' group and input that number in the new column, 'Agency_Count'\n",
    "#You can use any column identifier here if you are simply counting values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform function also allows us to do other calculations in groups such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Ag_Fine_Sum']= df.groupby(['Agency'])['FineAmount'].transform('sum') \n",
    "#will sum the values in FineAmount for each agency and input that value in the new column, 'Ag_Fine_Sum'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it makes more sense to do calculations on each row in a column based on a pre-identified condition. In order to do this we'll transform the data using __lambda__ functions.  Lambda functions run functions on each item in a column based on pre-set conditions. For example, if we want to create a new column that counts the number of citations over $ 500 that an Agency receives: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['fines_over_500'] = df.groupby(['Agency'])['FineAmount'].transform(lambda x: (x>500).count())\n",
    "df.sort_values('FineAmount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['fines_over_500'] = df.groupby(['Agency'])['FineAmount'].transform(lambda x: (x>500).count()) means: \n",
    "1. df['fines_over_500'] = : defines the new column in our dataframe that we want to make with our data\n",
    "2. df.groupby(['Agency']) : we want to group our dataframe the Agency type\n",
    "3. ['FineAmount'] : in each Agency group, we are going to do something with the FineAmount column\n",
    "4. .transform : we are going to transform our data in the FineAmount column by...\n",
    "5. (lambda x: (x>500).count()) : using a lambda function! This part of the code tells us that for every row (x) in the column we pre-identified, we will check if that value is greater than 500 (x>500).  If it is, then we'll count it; if it's not, then we won't count it (.count())\n",
    "6. This will fill in the number of citations issues over $ 500 per Agency, so all rows corresponding to a particular agency will have the same value in this column. This might be helpful if we want to perform column-to-column calculations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data grouping is great for helping us identify trends in our data, but if we scroll through the data, we'll see that this citation data occurs over several years. It might be useful to analyze this data for the entire time period provided, but we might also want to look at specific time periods such as years, months, quarters, etc.\n",
    "\n",
    "We can use another column operation to filter out specific dates, but we''ll first need to convert our \"time\" data columns to a datetime format (remember that when we looked at the df.info() almost everything was stored as an 'object'). If we want to filter information by the ViolationDate, we'll convert the data by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['ViolationDate'] = pd.to_datetime(df['ViolationDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can filter the data by ViolationDate. For example, if we want to look at the violation dates in 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[(df['ViolationDate']> '01/01/2013') & (df['ViolationDate']< '12/31/2013')]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __.loc__ allows us to sort through  or identify parts of our dataframe based on specific row labels. This function acts similarly to lambda in that the code \"looks\" at every row and checks whether or not it complies with our argument, and we can perform similar some of the same functions (see below), but this method can become inefficient when we have larger dataframes. \n",
    "\n",
    "When you try to run this code to perform the same task we did with the lambda function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['fines_over_200'] = df.loc[df['FineAmount']>200].count()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our code works and outputs a dataframe, it also gives us a \"SettingWithCopyWarning.\" This warning basically warns us that our code might not have worked as we expected based on how we re-formatted the dataframe, and even if we get the right output now, this might not be the case if we continue to edit our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
